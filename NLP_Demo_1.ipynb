{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of NLP Demo 1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python (base)",
      "language": "python",
      "name": "base"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajeev03/python0/blob/master/NLP_Demo_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MNzCiJPFKyk2"
      },
      "source": [
        "\n",
        "\n",
        "## Naive Bayes Classification of Sentiments of Text\n",
        "\n",
        "#### Probabilistic Model of Classification\n",
        "\n",
        "In a probabilistic classification model we want to estimate the value of \n",
        "$P(c|x)$\n",
        ", the probability of a sample x being of class c. Naive Bayes is one such probabilistic classifier that uses Bayes' Rule to classify samples. And Naive Bayes is _\"Naive\"_ because it assumes strong independence among all the features of sample x.\n",
        "\n",
        "#### Bayes Rule:\n",
        "\n",
        "$P(c|x) = \\frac{P(x|c)P(c)}{P(x)}$\n",
        "\n",
        "#### Text Classification using Naive Bayes classifier\n",
        "\n",
        "Consider the task of classifying textual documents into having positive or negative sentiments. We will design the Naive Bayes classifier for this problem as follows:\n",
        "\n",
        "Samples are text documents, and their features are the words that comprises these documents.\n",
        "\n",
        "- Each document $d$ is a sequence of words, $d = w_1w_2...w_n$, where $w_i$ are the tokens of the document and $n$ is the total number of tokens in the document $d$.\n",
        "\n",
        "- The training dataset consists of many document, sentiment pairs, ${d_i, s_i}$\n",
        "\n",
        "- Each document $d_i$ is associated with a sentiment $s_i \\in \\{0,1\\}$, $0$ being negative sentiment and $1$ being positive sentiment.\n",
        "\n",
        "Using **Bayes' Rule** we have \n",
        "\n",
        "$p(s|d) = \\frac{p(d|s)p(s)}{p(d|s)p(s) + p(d|\\bar{s})p(\\bar{s})}$\n",
        "\n",
        "And from the **independence assumption** of features\n",
        "\n",
        "$p(d|s) = p(w_1,w_2,..., w_n|s) = p(w_1|s)p(w_2|s)...p(w_n|s)$\n",
        "\n",
        "Also in the **IMDb reviews dataset** that we are considering here have equal number of positive and negative datasets.\n",
        "\n",
        "We have $p(s) = 0.5$ and $p(\\bar{s})=0.5$.\n",
        "\n",
        "This simplifies our formulation for \n",
        "$p(s|d)$\n",
        "\n",
        "$ p(s|d) = \\frac{p(d|s)}{p(d|s) + p(d|\\bar{s})} $\n",
        "\n",
        "If we assign threshold of \n",
        "$p_T(s|d) = 0.5$\n",
        "for deciding the final label, the model simplifies to,\n",
        "\n",
        "$y=\n",
        "    \\begin{cases}\n",
        "      1, & \\text{if } p(d|s=1) \\geq p(d|s=0)\\\\\n",
        "      0, & \\text{otherwise}\n",
        "    \\end{cases}$\n",
        "#### A measure for numerical stability\n",
        "\n",
        "$p(w_i)$ will be very small in magnitude, and when we take a product of such very small numbers to compute $p(d|s)$\n",
        ", even double precision floating points fail to store such small numbers and becomes zero. Hence, for numerical stability, we will convert the probabilities to log probability,\n",
        "\n",
        "$\\log p(d|s) = \\log p(w_1,w_2,..., w_n|s) = \\log p(w_1|s) + \\log p(w_2|s) + ...+ \\log p(w_n|s)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA7yB6r_mlTe",
        "colab_type": "text"
      },
      "source": [
        "### Basic NLP Tasks\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pgc5tG_TKykg",
        "colab": {}
      },
      "source": [
        "# Import 'os' for preliminary tasks like directory listing etc.\n",
        "import os\n",
        "\n",
        "# Import re for regex string matching\n",
        "import re\n",
        "\n",
        "# Import nltk for nlp\n",
        "import nltk\n",
        "\n",
        "# Import library providing high-performance, easy-to-use data structures and data analysis tools\n",
        "import pandas as pd\n",
        "\n",
        "# Import Python's native data structures Counter and defaultdict\n",
        "# Counter - maintains count of element\n",
        "# defaultdict - dictionary data structure with exception handling for missing keys\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "# Import tqdm for fancy progressbars!\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "# Import numpy for different mathematical operations on arrays / matrices\n",
        "import numpy as np\n",
        "\n",
        "from nltk.tokenize import word_tokenize # import tokenizer\n",
        "from nltk.corpus import stopwords # import stopwords\n",
        "from nltk.stem.porter import PorterStemmer #import stemmer\n",
        "from nltk.stem import WordNetLemmatizer #lemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from sklearn.feature_extraction.text import TfidfTransformer #\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # from text to vector\n",
        "from sklearn.naive_bayes import MultinomialNB #import naive bayes classifier\n",
        "from sklearn import svm #import SVM classifier\n",
        "from  sklearn.metrics  import accuracy_score # accuracy measure\n",
        "from sklearn.tree import DecisionTreeClassifier # Decision tree classfier\n",
        "from sklearn.ensemble import RandomForestClassifier # Random Forest Classfier "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GqYEfPkiNpvc",
        "colab": {}
      },
      "source": [
        "# Install the nltk component for several tasks\n",
        "nltk.download('punkt')     \n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzFv6VWRWKhm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sentence for testing\n",
        "#sentence= \"The quick brown fox jumps over the lazy dog\"\n",
        "#sentence= \"Backgammon is one of the oldest known board games\"\n",
        "sentence= \"It is better to use the corpora about the rocks\"\n",
        "\n",
        "#function to split text into word\n",
        "tokens = word_tokenize(sentence)\n",
        "print (tokens)\n",
        "\n",
        "#POS_Tagging\n",
        "nltk.pos_tag(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEqtlOcPWLh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#stop words removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "print (stop_words)\n",
        "tokens = [w for w in tokens if not w in stop_words]\n",
        "print(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOUP3zbXZRK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# stemming\n",
        "porter = PorterStemmer()\n",
        "stems = []\n",
        "for t in tokens:    \n",
        "    stems.append(porter.stem(t))\n",
        "print(stems)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga4oSKjrZgUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#lemmatizing\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmas=[]\n",
        "for t in tokens:\n",
        "  lemmas.append(lemmatizer.lemmatize(t))\n",
        "print (lemmas)\n",
        "\n",
        "print(\"better:\", lemmatizer.lemmatize(\"better\",pos =\"a\" ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7rskn91gKykl"
      },
      "source": [
        "### Downloading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V0vFdDGbKykm",
        "outputId": "de0d7bf6-7a7e-4dbc-cf4c-d4c96a4de2e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz -P data/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-20 06:37:50--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘data/aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  45.5MB/s    in 1.8s    \n",
            "\n",
            "2019-07-20 06:37:52 (45.5 MB/s) - ‘data/aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BAxLFB7oKykp"
      },
      "source": [
        "### Extract data\n",
        "Please wait for ~15s"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6m_dPDAoKykp",
        "outputId": "4794a9f4-53dd-412c-91bb-691ed48ddbd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "!tar -xzf data/aclImdb_v1.tar.gz -C data/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 81 ms, sys: 14 ms, total: 95 ms\n",
            "Wall time: 8.31 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcRfn_pKRUTc",
        "colab_type": "text"
      },
      "source": [
        "### Data Samples\n",
        "- Dataset is split into two parts for training and testing\n",
        "- Positive and negative samples are organized in individual folders \n",
        "- Each sample document is stored in a .txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncwqIYpHeHHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert the dataset from files to a python DataFrame\n",
        "folder = 'data/aclImdb/'\n",
        "labels = {'pos': 1, 'neg': 0} \n",
        "df = pd.DataFrame()\n",
        "revList = list()\n",
        "for f in ('test', 'train'):    \n",
        "    for l in ('pos', 'neg'):\n",
        "        path = os.path.join(folder, f, l)\n",
        "        for file in os.listdir (path) :\n",
        "            with open(os.path.join(path, file),'r', encoding='utf-8') as infile:\n",
        "                txt = infile.read()\n",
        "                revList.append((txt,labels[l]))\n",
        "            #df = df.append([[txt, labels[l]]],ignore_index=True)\n",
        "df = pd.DataFrame.from_records(revList)\n",
        "df.columns = ['review', 'sentiment']\n",
        "#df.head()\n",
        "#df.tail(50)\n",
        "#df.loc[27000, 'review']\n",
        "#df.loc[27000, 'sentiment']\n",
        "#df.loc[27500, 'sentiment']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXSN5c7bRcBL",
        "colab_type": "text"
      },
      "source": [
        "### Build Vocabulary\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SJtY9mw6Kykv",
        "colab": {}
      },
      "source": [
        "reviews = df.review.str.cat(sep=' ')#function to split text into word\n",
        "tokens = word_tokenize(reviews)\n",
        "vocabulary = set(tokens)\n",
        "print(len(vocabulary))\n",
        "#frequency_dist = nltk.FreqDist(tokens)\n",
        "#sorted(frequency_dist,key=frequency_dist.__getitem__, reverse=True)[0:50]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rtmF1HXr4l5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "vocabulary = [w for w in vocabulary if not w in stop_words]\n",
        "print (len (vocabulary))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFRztCa2Rn0P",
        "colab_type": "text"
      },
      "source": [
        "###Build Classifier\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E63RiYjt3ZMr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#building a classifier\n",
        "X_train = df.loc[:24999, 'review'].values\n",
        "y_train = df.loc[:24999, 'sentiment'].values\n",
        "X_test = df.loc[25000:, 'review'].values\n",
        "y_test = df.loc[25000:, 'sentiment'].values\n",
        "vectorizer = TfidfVectorizer()\n",
        "#v=vectorizer.fit_transform(vocabulary)\n",
        "train_vectors=vectorizer.fit_transform(X_train)\n",
        "#train_vectors = vectorizer.transform(X_train)\n",
        "test_vectors = vectorizer.transform(X_test)\n",
        "print(train_vectors.shape, test_vectors.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgzM1ceGsuZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#fit the classifier Naive Bayes\n",
        "clf = MultinomialNB().fit(train_vectors, y_train)\n",
        "predicted = clf.predict(test_vectors)\n",
        "print(accuracy_score(y_test,predicted))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZDEHP7ZtjmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#clf = svm.SVC().fit(train_vectors, y_train)\n",
        "clf = DecisionTreeClassifier(max_depth=5).fit(train_vectors, y_train)\n",
        "#clf=RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1).fit(train_vectors, y_train)\n",
        "predicted = clf.predict(test_vectors)\n",
        "print(accuracy_score(y_test,predicted))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDMwyV6AQo-s",
        "colab_type": "text"
      },
      "source": [
        "### Implementation of Naive Bayes Classifier without Using Library Function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mJOwy_iyKyky",
        "colab": {}
      },
      "source": [
        "data_folder = 'data/aclImdb/'\n",
        "\n",
        "rp = os.path.join(data_folder, 'train/pos')\n",
        "train_positive = [os.path.join(rp, f) for f in os.listdir(rp)]\n",
        "rp = os.path.join(data_folder, 'train/neg')\n",
        "train_negative = [os.path.join(rp, f) for f in os.listdir(rp)]\n",
        "\n",
        "rp = os.path.join(data_folder, 'test/pos')\n",
        "test_positive = [os.path.join(rp, f) for f in os.listdir(rp)]\n",
        "rp = os.path.join(data_folder, 'test/neg')\n",
        "test_negative = [os.path.join(rp, f) for f in os.listdir(rp)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PXyAkzlcKyk3"
      },
      "source": [
        "### Regex for cleaning html tags\n",
        "- Pattern <.*?> means \"anything within two angular brackets\". The qualifier *? denotes \"as few times as possible\". This makes sure we match only one html tag at a time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TTY671XQKyk4",
        "colab": {}
      },
      "source": [
        "re_html_cleaner = re.compile(r\"<.*?>\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Pmq06cnLKyk7"
      },
      "source": [
        "#### Limit number of samples\n",
        "To quickly train a small model, consider setting n_train and n_test to some relatively small numbers e.g. `1000`. Set, \n",
        "`n_train = n_test = -1` to use all the samples available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-kTiqMYYKyk8",
        "colab": {}
      },
      "source": [
        "n_train = 2500\n",
        "n_test = 2500"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VdJ5ALiQKylA"
      },
      "source": [
        "### (Conditional) Unigram Counter\n",
        "- Calculates the distribution $p(w|s=1)$ and $p(w|s=0)$, empirically, from training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A-NMnbQaKylB",
        "colab": {}
      },
      "source": [
        "# Distribution of word tokens in positive samples\n",
        "positive_word_counts = Counter()\n",
        "\n",
        "for _fname in tqdm_notebook(train_positive[:n_train], desc=\"Crunching +ve samples: \"):\n",
        "    with open(_fname) as f:\n",
        "        text = f.read().strip()\n",
        "        text = re_html_cleaner.sub(\" \", text)\n",
        "        positive_word_counts += Counter(nltk.word_tokenize(text))\n",
        "\n",
        "# Distribution of word tokens in negative samples\n",
        "negative_word_counts = Counter()\n",
        "\n",
        "for _fname in tqdm_notebook(train_negative[:n_train], desc=\"Crunching -ve samples: \"):\n",
        "    with open(_fname) as f:\n",
        "        text = f.read().strip()\n",
        "        text = re_html_cleaner.sub(\" \", text)\n",
        "        negative_word_counts += Counter(nltk.word_tokenize(text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LZqkE8xzKylH"
      },
      "source": [
        "#### Unigram counts to probability distribution\n",
        "\n",
        "$p(w|s) = \\frac{N_{s,w}}{N_{s,*}} = \\frac{N_{s,w}}{\\sum_{w' \\in W}N_{s,w'}}$\n",
        "\n",
        "#### Additive Smoothing\n",
        "- Note that, if some token, $u$, unseen in training documents, occurrs in a test document, $p(doc_{test}|s)$ becomes $0$ as $N_{s,u}$ for that token is $0$.\n",
        "- We apply _Additive Smoothing_ to prevent probability from going to zero.\n",
        "\n",
        "$p(w|s) = \\frac{\\alpha + N_{s,w}}{\\sum_{w' \\in W}(\\alpha + N_{s,w'})} = \\frac{\\alpha + N_{s,w}}{\\alpha V + \\sum_{w' \\in W}N_{s,w'}}$\n",
        "\n",
        "where V is the total vocab size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mwWDtXqdKylH",
        "colab": {}
      },
      "source": [
        "len_corpus_pos = sum(positive_word_counts.values())\n",
        "len_corpus_neg = sum(negative_word_counts.values())\n",
        "V_pos = len(positive_word_counts)\n",
        "V_neg = len(negative_word_counts)\n",
        "alpha = 0.1\n",
        "log_p_vocab_pos = defaultdict(\n",
        "    lambda: np.log(alpha/len_corpus_pos), \n",
        "    {w:np.log((alpha + c)/(V_pos*alpha + len_corpus_pos)) for w,c in positive_word_counts.items()}\n",
        ")\n",
        "log_p_vocab_neg = defaultdict(\n",
        "    lambda: np.log(alpha/len_corpus_neg), \n",
        "    {w:np.log((alpha + c)/(V_neg*alpha + len_corpus_neg)) for w,c in negative_word_counts.items()}\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8iud_C9nKylK",
        "outputId": "f20a7438-751c-4d4d-bb3a-e313a0ff04c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "p_data_pos = len(train_positive)/(len(train_positive) + len(train_negative))\n",
        "print(f\"Prob. of +ve sentiment in our dataset: {p_data_pos}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prob. of +ve sentiment in our dataset: 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FMqVNMaUKylN"
      },
      "source": [
        "#### get_prob_pos(doc)\n",
        "\n",
        "A function that accepts a document string as input, tokenizes it and computes the probability \n",
        "$p(d|s=1)$\n",
        "and \n",
        "$p(d|s=0)$\n",
        ". It returns 1 if \n",
        "$p(d|s=1) \\geq p(d|s=0)$ \n",
        "otherwise 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iQZ8i40BKylP",
        "colab": {}
      },
      "source": [
        "def get_prob_pos(doc):\n",
        "    text = doc.strip()\n",
        "    text = re_html_cleaner.sub(\" \", text)\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    p_pos = 1\n",
        "    p_neg = 1\n",
        "    for token in tokens:\n",
        "        p_pos += log_p_vocab_pos[token]\n",
        "        p_neg += log_p_vocab_neg[token]\n",
        "        \n",
        "    return 1.0*(p_pos >= p_neg) #/(p_pos+p_neg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZKRyUJICKylS",
        "colab": {}
      },
      "source": [
        "results = []\n",
        "for _fname in tqdm_notebook(test_positive[:n_test], desc=\"Classifying test data: \"):\n",
        "    with open(_fname) as f:\n",
        "        results.append((1, get_prob_pos(f.read())))\n",
        "        \n",
        "\n",
        "for _fname in tqdm_notebook(test_negative[:n_test], desc=\"Classifying test data: \"):\n",
        "    with open(_fname) as f:\n",
        "        results.append((0, get_prob_pos(f.read())))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fo2WqwChKylY"
      },
      "source": [
        "### Performance evaluation of our model\n",
        "\n",
        "**Accuracy:** Overall performance of our model, fraction of samples that were labelled correctly\n",
        "\n",
        "**Recall:** Out of all +ve data samples in test set, what fraction of it was labelled correctly\n",
        "\n",
        "**Precision:** How precise is the model? Out of all samples that were tagged +ve by the model, how many were actually positive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wrmPCo4WKylV",
        "colab": {}
      },
      "source": [
        "true_pos = 0\n",
        "false_pos = 0\n",
        "true_neg = 0\n",
        "false_neg = 0\n",
        "for true_label, pred_label in results:\n",
        "    if true_label == 1 and pred_label == 1:\n",
        "        true_pos += 1\n",
        "    elif true_label == 1 and pred_label == 0:\n",
        "        false_neg += 1\n",
        "    elif true_label == 0 and pred_label == 1:\n",
        "        false_pos += 1\n",
        "    elif true_label == 0 and pred_label == 0:\n",
        "        true_neg += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9JbxE8PMKylZ",
        "outputId": "256b17a9-e687-49a5-e8b1-34c75b30f38f",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "print(f\"Accuracy: {(true_pos + true_neg)/(true_pos + true_neg + false_pos + false_neg):0.4F}\")\n",
        "print(f\"Recall: {(true_pos)/(true_pos + false_neg):0.4F}\")\n",
        "print(f\"Precision: {(true_pos)/(true_pos + false_pos):0.4F}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7904\n",
            "Recall: 0.7416\n",
            "Precision: 0.8218\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TcaT4_tkKylc",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}